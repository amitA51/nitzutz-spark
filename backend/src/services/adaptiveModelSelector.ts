import { createAIClientFromAny, chatCompletion } from './aiClient';
import { contentCache } from './contentCache';

interface ModelConfig {
  name: string;
  provider: string;
  strengths: string[];
  weaknesses: string[];
  maxTokens: number;
  costPerToken: number;
  speed: 'fast' | 'medium' | 'slow';
  complexity: 'light' | 'medium' | 'heavy';
  languages: string[];
  specialties: string[];
}

interface TaskRequirements {
  taskType: 'content_generation' | 'question_answering' | 'analysis' | 'translation' | 'coding' | 'creative';
  complexity: 'simple' | 'medium' | 'complex';
  outputLength: 'short' | 'medium' | 'long';
  language: 'hebrew' | 'english' | 'mixed';
  quality: 'draft' | 'standard' | 'premium';
  urgency: 'low' | 'medium' | 'high';
  context?: string;
}

export class AdaptiveModelSelector {
  private availableModels: ModelConfig[] = [
    {
      name: 'deepseek-ai/DeepSeek-V3.2-Exp',
      provider: 'huggingface',
      strengths: ['reasoning', 'code_generation', 'math', 'analysis'],
      weaknesses: ['creative_writing', 'casual_chat'],
      maxTokens: 8192,
      costPerToken: 0.0001,
      speed: 'medium',
      complexity: 'heavy',
      languages: ['hebrew', 'english'],
      specialties: ['technical_content', 'educational', 'problem_solving']
    },
    {
      name: 'microsoft/DialoGPT-medium',
      provider: 'huggingface',
      strengths: ['conversation', 'quick_responses', 'casual_tone'],
      weaknesses: ['complex_reasoning', 'long_content'],
      maxTokens: 1024,
      costPerToken: 0.00001,
      speed: 'fast',
      complexity: 'light',
      languages: ['english'],
      specialties: ['chat', 'simple_qa']
    },
    {
      name: 'meta-llama/Llama-3.2-11B-Vision',
      provider: 'huggingface',
      strengths: ['multimodal', 'creative_writing', 'storytelling'],
      weaknesses: ['mathematical_reasoning', 'code'],
      maxTokens: 4096,
      costPerToken: 0.00005,
      speed: 'medium',
      complexity: 'medium',
      languages: ['hebrew', 'english'],
      specialties: ['creative_content', 'narrative', 'visual_description']
    }
  ];

  /**
   * ×‘×—×™×¨×ª ×”××•×“×œ ×”××ª××™× ×‘×™×•×ª×¨ ×œ××©×™××”
   */
  async selectBestModel(requirements: TaskRequirements): Promise<ModelConfig> {
    console.log(`ğŸ¤– [Adaptive Model] Selecting model for ${requirements.taskType} task...`);
    
    // ×‘×“×™×§×ª cache ×œ××•×“×œ ×©× ×‘×—×¨ ×¢×‘×•×¨ ××©×™××” ×“×•××”
    const cacheKey = `selected_model_${JSON.stringify(requirements)}`;
    const cachedSelection = contentCache.get<ModelConfig>(cacheKey);
    if (cachedSelection) {
      console.log(`ğŸ“¦ [Adaptive Model] Using cached model selection: ${cachedSelection.name}`);
      return cachedSelection;
    }

    // ×—×™×©×•×‘ ×¦×™×•×Ÿ ×”×ª×××” ×œ×›×œ ××•×“×œ
    const scoredModels = this.availableModels.map(model => ({
      model,
      score: this.calculateModelScore(model, requirements)
    }));

    // ××™×•×Ÿ ×œ×¤×™ ×¦×™×•×Ÿ ×”×ª×××”
    scoredModels.sort((a, b) => b.score - a.score);
    
    const selectedModel = scoredModels[0].model;
    const topScore = scoredModels[0].score;

    console.log(`âœ… [Adaptive Model] Selected ${selectedModel.name} with score ${topScore.toFixed(2)}`);
    
    // ×©××™×¨×” ×‘cache ×œ×©×™××•×© ×¢×ª×™×“×™ (10 ×“×§×•×ª)
    contentCache.set(cacheKey, selectedModel, 10 * 60 * 1000);
    
    // ×œ×•×’ ×”×”×—×œ×˜×” ×œ× ×™×ª×•×— ×¢×ª×™×“×™
    await this.logModelSelection(requirements, selectedModel, topScore, scoredModels);
    
    return selectedModel;
  }

  /**
   * ×—×™×©×•×‘ ×¦×™×•×Ÿ ×”×ª×××” ×©×œ ××•×“×œ ×œ××©×™××”
   */
  private calculateModelScore(model: ModelConfig, requirements: TaskRequirements): number {
    let score = 0;
    
    // ×‘×“×™×§×ª ×”×ª×××” ×œ×¡×•×’ ×”××©×™××”
    score += this.getTaskTypeScore(model, requirements.taskType);
    
    // ×‘×“×™×§×ª ×”×ª×××” ×œ××•×¨×›×‘×•×ª
    score += this.getComplexityScore(model, requirements.complexity);
    
    // ×‘×“×™×§×ª ×”×ª×××” ×œ××•×¨×š ×”×¤×œ×˜
    score += this.getOutputLengthScore(model, requirements.outputLength);
    
    // ×‘×“×™×§×ª ×”×ª×××” ×œ×©×¤×”
    score += this.getLanguageScore(model, requirements.language);
    
    // ×‘×“×™×§×ª ××™×›×•×ª × ×“×¨×©×ª vs ×™×›×•×œ×ª ×”××•×“×œ
    score += this.getQualityScore(model, requirements.quality);
    
    // ×‘×“×™×§×ª ×“×—×™×¤×•×ª (××”×™×¨×•×ª)
    score += this.getUrgencyScore(model, requirements.urgency);
    
    // ×‘×•× ×•×¡ ×¢×‘×•×¨ ×”×ª××—×•×™×•×ª ××™×•×—×“×•×ª
    score += this.getSpecialtyScore(model, requirements);
    
    return Math.max(0, score);
  }

  private getTaskTypeScore(model: ModelConfig, taskType: string): number {
    const taskToStrengthMap: Record<string, string[]> = {
      'content_generation': ['reasoning', 'creative_writing'],
      'question_answering': ['reasoning', 'quick_responses'],
      'analysis': ['reasoning', 'math', 'analysis'],
      'translation': ['multilingual', 'language'],
      'coding': ['code_generation', 'reasoning'],
      'creative': ['creative_writing', 'storytelling']
    };
    
    const relevantStrengths = taskToStrengthMap[taskType] || [];
    const matchingStrengths = model.strengths.filter(strength => 
      relevantStrengths.includes(strength)
    );
    
    return matchingStrengths.length * 20; // 20 × ×§×•×“×•×ª ×œ×›×œ ×—×•×–×§ ×¨×œ×•×•× ×˜×™
  }

  private getComplexityScore(model: ModelConfig, complexity: string): number {
    const complexityMap: Record<string, number> = { 'simple': 1, 'medium': 2, 'complex': 3 };
    const modelMap: Record<string, number> = { 'light': 1, 'medium': 2, 'heavy': 3 };
    
    const taskComplexity = complexityMap[complexity] || 2;
    const modelComplexity = modelMap[model.complexity] || 2;
    
    // ××•×“×œ×™× ×›×‘×“×™× ×˜×•×‘×™× ×œ××©×™××•×ª ××•×¨×›×‘×•×ª, ××•×“×œ×™× ×§×œ×™× ×˜×•×‘×™× ×œ××©×™××•×ª ×¤×©×•×˜×•×ª
    const diff = Math.abs(taskComplexity - modelComplexity);
    return Math.max(0, 30 - diff * 10);
  }

  private getOutputLengthScore(model: ModelConfig, outputLength: string): number {
    const lengthRequirements: Record<string, number> = {
      'short': 500,
      'medium': 2000,
      'long': 8000
    };
    
    const requiredTokens = lengthRequirements[outputLength] || 2000;
    
    if (model.maxTokens >= requiredTokens) {
      return 20;
    } else if (model.maxTokens >= requiredTokens * 0.7) {
      return 10;
    }
    
    return -10; // ×¢×•× ×© ×× ×”××•×“×œ ×œ× ×™×›×•×œ ×œ×˜×¤×œ ×‘××•×¨×š ×”× ×“×¨×©
  }

  private getLanguageScore(model: ModelConfig, language: string): number {
    if (model.languages.includes(language)) {
      return 25;
    } else if (language === 'mixed' && model.languages.length > 1) {
      return 20;
    } else if (model.languages.includes('english') && language === 'hebrew') {
      return 5; // ×¦×™×•×Ÿ × ××•×š ××‘×œ ×œ× ××¤×¡
    }
    
    return 0;
  }

  private getQualityScore(model: ModelConfig, quality: string): number {
    const qualityToComplexityMap: Record<string, string> = {
      'draft': 'light',
      'standard': 'medium', 
      'premium': 'heavy'
    };
    
    const neededComplexity = qualityToComplexityMap[quality];
    
    if (model.complexity === neededComplexity) {
      return 30;
    } else if (model.complexity === 'heavy' && quality !== 'draft') {
      return 20; // ××•×“×œ×™× ×›×‘×“×™× ×ª××™×“ ×˜×•×‘×™× ×œ××™×›×•×ª
    }
    
    return 10;
  }

  private getUrgencyScore(model: ModelConfig, urgency: string): number {
    const urgencyToSpeedMap: Record<string, string> = {
      'high': 'fast',
      'medium': 'medium',
      'low': 'slow'
    };
    
    const neededSpeed = urgencyToSpeedMap[urgency];
    
    if (model.speed === neededSpeed) {
      return 20;
    } else if (urgency === 'high' && model.speed === 'fast') {
      return 25; // ×‘×•× ×•×¡ ×¢×‘×•×¨ ××”×™×¨×•×ª ×‘×“×—×™×¤×•×ª ×’×‘×•×”×”
    } else if (urgency === 'low' && model.speed === 'slow') {
      return 15; // ×œ× × ×•×¨× ×©×”××•×“×œ ××™×˜×™ ×× ××™×Ÿ ×“×—×™×¤×•×ª
    }
    
    return 5;
  }

  private getSpecialtyScore(model: ModelConfig, requirements: TaskRequirements): number {
    let score = 0;
    
    // ×‘×“×™×§×” ×× ×™×© ×”×ª××—×•×ª ×¨×œ×•×•× ×˜×™×™×ª ×‘××•×“×œ
    const context = requirements.context?.toLowerCase() || '';
    
    for (const specialty of model.specialties) {
      if (context.includes(specialty.replace('_', ' '))) {
        score += 15;
      }
    }
    
    // ×‘×•× ×•×¡ ×¢×‘×•×¨ ××•×“×œ×™× ×—×™× ×•×›×™×™× ×× ×–×” ×ª×•×›×Ÿ ×—×™× ×•×›×™
    if (requirements.taskType === 'content_generation' && 
        model.specialties.includes('educational')) {
      score += 20;
    }
    
    return score;
  }

  /**
   * ×ª×™×¢×•×“ ×”×—×œ×˜×ª ×‘×—×™×¨×ª ×”××•×“×œ ×œ×× ×œ×™×˜×™×§×”
   */
  private async logModelSelection(
    requirements: TaskRequirements,
    selectedModel: ModelConfig,
    score: number,
    allScores: Array<{model: ModelConfig, score: number}>
  ): Promise<void> {
    try {
      const logData = {
        timestamp: new Date(),
        requirements,
        selectedModel: selectedModel.name,
        score,
        alternativeModels: allScores.slice(1, 3).map(s => ({
          name: s.model.name,
          score: s.score
        })),
        decisionFactors: {
          taskType: requirements.taskType,
          complexity: requirements.complexity,
          language: requirements.language,
          quality: requirements.quality
        }
      };
      
      console.log('ğŸ“Š [Adaptive Model] Decision logged:', {
        selected: selectedModel.name,
        score: score.toFixed(2),
        for: requirements.taskType
      });
      
    } catch (error) {
      console.warn('âš ï¸ [Adaptive Model] Failed to log model selection:', error);
    }
  }

  /**
   * ×™×¦×™×¨×ª AI client ×¢× ×”××•×“×œ ×©× ×‘×—×¨
   */
  async createOptimizedAIClient(requirements: TaskRequirements): Promise<any> {
    const selectedModel = await this.selectBestModel(requirements);
    
    // × ×™×ª×Ÿ ×œ×”×¨×—×™×‘ ×›××Ÿ ×œ×™×¦×•×¨ clients ×©×•× ×™× ×œ×¤×™ provider
    const client = await createAIClientFromAny();
    
    if (client) {
      // ×”×•×¡×¤×ª metadata ×¢×œ ×”××•×“×œ ×©× ×‘×—×¨
      (client as any).selectedModel = selectedModel;
      (client as any).modelName = selectedModel.name;
    }
    
    return client;
  }

  /**
   * ×§×‘×œ×ª ×”××œ×¦×” ×œ××•×“×œ ×œ×œ× ×™×¦×™×¨×ª client
   */
  async getModelRecommendation(requirements: TaskRequirements): Promise<{
    recommended: ModelConfig;
    alternatives: ModelConfig[];
    reasoning: string[];
  }> {
    const selected = await this.selectBestModel(requirements);
    
    const alternatives = this.availableModels
      .filter(m => m.name !== selected.name)
      .sort((a, b) => this.calculateModelScore(b, requirements) - this.calculateModelScore(a, requirements))
      .slice(0, 2);
    
    const reasoning = [
      `Selected for ${requirements.taskType} with ${requirements.complexity} complexity`,
      `Optimized for ${requirements.language} language`,
      `Quality level: ${requirements.quality}`,
      `Speed requirement: ${requirements.urgency}`,
    ];
    
    return {
      recommended: selected,
      alternatives,
      reasoning
    };
  }
}

// ×™×¦×™×¨×ª instance ×™×—×™×“
export const adaptiveModelSelector = new AdaptiveModelSelector();